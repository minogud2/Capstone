{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc. AI\n",
    "## Capstone Project\n",
    "### Darragh Minogue\n",
    "\n",
    "### 1. Background\n",
    "\n",
    "\n",
    "This project adopts the facility location problem to the school network in Ethiopia to expand access to secondary education. Of the 40,776 schools in the country, 37,039 offer primary and middle school education (grades 1-8), but only 3,737 offer secondary (grades 10-12). On average, middle schools are located more than 6km away from the nearest secondary school, meaning children face long journeys or they don't access secondary at all.\n",
    "\n",
    "The expansion of secondary education is a major priority for the Ministry of Education, Ethiopia. Ambitious five year targets are set in the Education Sector Development Plans (ESDP). In 2013/2014, the Gross Enrollment Rate (GER) for lower secondary was 38.9% (3,466,972/8,914,837 children) and a target of 74% (6,596,979/8,914,837 children) was set. However, by 2019/2020, the actual GER was 51.05% (4,551,024/8,914,837 children), a more modest increase of 12.15% rather than 35.1%.  In the latest five year ESDP (2020/2021-2024/2025), an increase of 24% was set, but under the key-performance indicator of 'Number of newly established secondary schools', it says TBD- for 'To Be Determined'. Therein lies part of the problem: planning. Often, a portion of the budget is allocated for secondary, then each region decides how and where to spend it. Planners then, at the macro level, identify primary schools with the highest demand (i.e. largest enrollment) and seek to construct schools to serve that particular school population. While this can often yeild good results, the goal of this project is to improve on this approach in a way that allows for the potential construction of secondary schools across a catchment of multiple primary schools, if enrollment is sufficient and distance is minimal. Meta-heuristic optimisation techniques are used to achieve this.\n",
    "\n",
    "The project makes use of readily available data from the Ministry of Education, Ethiopia, mainly 1) enrollment data from the Education Managament Information System (EMIS) and 2) geolocation data, recently obtained from the World Bank in 2020. Since national population and census data is inaccurate and outdated in Ethiopia, this was not considered a reliable source for this project. Instead, it is assumed that middle schools are located within the community and new secondary schools should therefore be constructed close to existing middle schools.\n",
    "\n",
    "### 2. The Problem\n",
    "\n",
    "In the problem, there is a list of existing middle schools and secondary schools in Ethiopia, their enrollment and location data (region, zone, district, longitude and latitude). The aim is to expand the secondary school network by constructing new secondary schools that best serve the demand from the existing middle school network. No limit is placed on capacity as this is determined by budget which is not known in advance and subject to change. Key to solving the problem is understanding that when a school is located within a village, there is no impact of distance on enrolment nor dropout. However, beyond the aspirational norm of 1-2km, distance affects initial access to school but also creates barriers to retention, completion and transition to higher level. As such, the aim is to minimise distance, but construct schools in locations with the highest demand.\n",
    "\n",
    "### 3. Approach\n",
    "\n",
    "<b> Glossary of terms</b>\n",
    "* MS: Middle Schools\n",
    "* SS: Secondary Schools\n",
    "* EE: Expected Enrollment\n",
    "* EEI: Expected Enrollment Increase\n",
    "* Feeder: a middle school that is part of the secondary school catchment area.\n",
    "* x: proposed new secondary schools (the genotype).\n",
    "* d: distance in km between primary schools\n",
    "* catchment: a middle school is considered within a catchment if it's \n",
    "* n: total number of proposed new SS to construct\n",
    "\n",
    "This solution proposes new SS locations that can provide the highest estimated enrollment from feeder MS whilst also ensuring they are constructed within at minimal distance. Given the multitude of different languages and the decentralised Government of Ethiopia, models are developed at a regional level.\n",
    "\n",
    "Two different algorithms are used in this project to find an optimal solution: Random Search, and CMA-ES. In each of the algorithms, the objective function f() aims to <b> maximise overall expected enrollment </b> given a set of feasible locations: x. Feasibility of the locations or the genotype, x, is controlled using a box boundary of longitudes and latitudes for a given region. Initial starting points are also provided within the box boundary using a generate_random_sp() helper function. \n",
    "\n",
    "The fitness is determined using an expected_enrollment function. This takes in the parameters: 1) the location of existing MS, 2) the enrollment of existing MS, 3) x, and 4) the current distance between existing MS and existing SS. The function then calculates the haversine distance in kilometers (km) between each new SS in x and all MS within a given region. This vector, <b> d </b>, is of size: (proposed_schools, len(ps)). Using this data, the algorithm then addresses three key challenges:\n",
    "\n",
    "1) **The need to identify the MS with the minimal distance to the new SS**. In this case, if a MS is close to two or more new SS, only the closest is selected. But if the MS is closer to an existing SS, then it's ignored as it's already a feeder school to the existing SS.\n",
    "\n",
    "2) **The need to estimate the expected enrollment of the new SS, based on the distance to its feeder MS**. If a school is beyond a certain threshold distance in km, then the MS should not be considered a feeder school for a SS. To handle this, situation, a helper function is used to estimate the expected enrollment from a feeder MS to the nearby SS. It takes in as parameters a) distance to nearest SS and b) MS enrollment. It then makes some assumptions about distance to return the expected enrollment per school. Theunynck (2014) recommends the adoptation of a norm of 2 or 3km for junior secondary schools and the function therefore assumes that if a school is constructed within 3km of a MS, there is no negative effect on expected enrollment of it's nearby SS. Above 3km, a linear dropoff is assumed between 3-5km. If a MS is located more than 5km away from the new location proposed, it is expected that zero students will attend from that feeder school.\n",
    "\n",
    "3) **The need to divert expected enrollment from a MS if it is currently a feeder school to an existing SS but the new SS proposed is closer**. This is dealt with by subtracting the expected enrollment from the MS to the existing SS using the shape function, and then allocating it to the new SS which is closer. \n",
    "\n",
    "A fitness evaluation budget of 10,000 is set to ensure there are sufficient iterations to achieve convergance. Each algorithm is run 30 times with different random starting points. The results are stored in a csv file, with the top 4 results plotted below. \n",
    "\n",
    "### Assumptions and Limitations\n",
    "\n",
    "1. It is assumed that 1km is equivalent to a 15 minute walk for children (Theunynck, S. 2014: p6).\n",
    "2. Distance is calculated using the haversine function and as a result distance is calculated as a straight line. The travel distances could therefore be further. Ethiopia is not well mapped and since most children are walking to school using other means like Google Maps API don't yeild useful results on a large scale and don't factor in more informal walking routes. Final results require close inspection for elevation and other issues that might impact walking distance or construction e.g. buildings, rivers. The final results should therefore be observed as an approximation and using a tool like ArcGIS or QGIS, the the results are observed for these types of obstacles. \n",
    "3. It is assumed that children beyond 5km are not likely to attend, but in some cases, this is not true. Some children walk extremely long distances to attend secondary school, while others stay with relatives or family friends to attend a school that is beyond 5km. Despite this being a reality, this shouldn't guide the construction as the goal is to minimise distance and create more equitable access to secondary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from haversine import haversine, haversine_vector, Unit # for distance\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cma\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "# Supress the scientific notation on numpy for easierx reading.\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean_dataset_final.csv', converters={'point': pd.eval}) # read in cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Afar as only 5% of schools are mapped.\n",
    "df = df[df['ADM1_EN'] != 'Afar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(distance, enrollment):\n",
    "    \"\"\" Returns the expected enrollment based on distance.\n",
    "    \n",
    "        If distance is less than 3km, all children are expected to contribute to the supply of a new school. \n",
    "        The full enrollment is returned. If above 5km, the school is considered too far and no children contribute to \n",
    "        the supply. Zero enrollment is returned. If between 3-5km, a linear dropoff is assumed. e.g. for 100 children\n",
    "        at a distance of 4km, 50 is returned. \n",
    "\n",
    "        Parameters:\n",
    "            distance (float): the distance in km of a school to the next level school.\n",
    "            enrollment (int): the enrollment of the school.\n",
    "        \n",
    "        Returns:\n",
    "            shaped_enroll(float): expected feeder enrollment from one school.\n",
    "    \n",
    "    \"\"\"\n",
    "    min_walk = 3 # minimum km walking distance\n",
    "    max_walk = 5 # maximum km walking distance\n",
    "    shaped_enroll = np.where(distance < min_walk, enrollment,\n",
    "             np.where(distance>max_walk, 0, enrollment*(1-(distance-min_walk)/(max_walk-min_walk))))\n",
    "    return shaped_enroll\n",
    "\n",
    "def generate_random_sp(bounds):\n",
    "    \"\"\"  Function to generate a vector of 40000 random (lat, lon) starting points for each \n",
    "         proposed school within box boundary.\n",
    "    \"\"\"\n",
    "    lat = np.random.uniform(low=bounds[0][0], high=bounds[1][0], size=40000)\n",
    "    lon = np.random.uniform(low=bounds[0][1], high=bounds[1][1], size=40000)\n",
    "    sp = np.vstack((lat, lon)).T\n",
    "    return sp\n",
    "\n",
    "def check_region(vec, region_shp):\n",
    "    \"\"\" Function to check if initial vector of starting points are located within the regional boundaries/polygon. \n",
    "        Returns boolean vector\n",
    "    \"\"\"\n",
    "    vec = gpd.points_from_xy(vec[:, 1], vec[:, 0]) # lat = y, x=lon\n",
    "    return vec.within(region_shp[0])\n",
    "\n",
    "def generate_sp_proposed(sp_list, proposed_schools):\n",
    "    \"\"\" Returns a vector of random starting points of size (proposed_schools)\n",
    "    \"\"\"\n",
    "    return sp_list[np.random.choice(sp_list.shape[0], proposed_schools,replace=False)]\n",
    "\n",
    "def get_bounds(df_):\n",
    "    \"\"\" Returns the minimum and maximum box boundary for school construction.\n",
    "    \"\"\"\n",
    "    bounds = np.array([[np.min(df_['lat']), np.min(df_['lon'])], [np.max(df_['lat']), np.max(df_['lon'])]])\n",
    "    return bounds\n",
    "\n",
    "def get_cma_bounds(bounds, n):\n",
    "    \"\"\" CMA-ES expects a list of size 2 for bounds. This function returns the bounds reshaped for CMA-ES\n",
    "    \"\"\"\n",
    "    x1y1 = np.repeat([bounds[0,:]],n, axis=0).flatten()\n",
    "    x2y2 = np.repeat([bounds[1,:]],n, axis=0).flatten()\n",
    "    boundsxy = [x1y1,x2y2]\n",
    "    return boundsxy\n",
    "\n",
    "def create_results_table():\n",
    "    \"\"\" Function that checks to see if a results csv exists, it creates the csv.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('./results.csv') == False:\n",
    "        results = pd.DataFrame(columns=['region','proposed_schools_n','starting_point', 'algorithm', 'ee',\n",
    "                                        'eei', 'proposed_locations', 'time', 'sigma'])\n",
    "        results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(df_, region):\n",
    "    \"\"\" A function which prepares datasets for the experiments.\n",
    "    \"\"\"\n",
    "    df_ = df_.loc[df_['ADM1_EN'] == region] # filter dataset by region\n",
    "    \n",
    "    # Create subset numpy arrays for the Fitness Function.\n",
    "    # 1. Full dataset filtered to only middle schools. \n",
    "    # 2. df_middle_enroll: MS enrollment data. Only the last two grades used as predictors for expected SS enrollment\n",
    "    # 3. df_middle_loc: MS location data- lat lon point data. \n",
    "    # 4. df_sec_enroll: SS enrollment data. Only grades 9 and 10 enrollment.\n",
    "    # 5. df_sec_loc: SS location data- lat lon point data. \n",
    "    # 6. current_ms_distance: existing distances from MS to SS\n",
    "\n",
    "    df_middle = df_.loc[df_['grade_7_8'] > 0] # 1\n",
    "    df_middle_enroll = df_middle['grade_7_8'].reset_index(drop=True).to_numpy(dtype=float) # 2\n",
    "    df_middle_loc = df_middle['point'].reset_index(drop=True).to_numpy()\n",
    "    df_middle_loc = np.array([np.array(i) for i in df_middle_loc], dtype=float) # 3\n",
    "\n",
    "    df_sec = df_.loc[ (df_['gr_offer'] == 'G. 9-10') | (df_['gr_offer'] == 'G. 9-12')]\n",
    "    df_sec_enroll = df_sec['grade9_10'].reset_index(drop=True).to_numpy(dtype=float) # 4\n",
    "    df_sec_loc = df_sec['point'].reset_index(drop=True).to_numpy() \n",
    "    df_sec_loc = np.array([np.array(i) for i in df_sec_loc], dtype=float) # 5\n",
    "    current_ms_distance = df_middle['nearest_lwr_sec'].to_numpy() # 6\n",
    "    \n",
    "    return [df_middle, df_middle_enroll, df_middle_loc, df_sec_enroll, df_sec_loc, current_ms_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regional datasets in a dictionary.\n",
    "datasets = {}\n",
    "for i in set(df['ADM1_EN']):\n",
    "    datasets[i] = prepare_datasets(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def benchmarks(region, data, n):\n",
    "#     # *** ESTABLISH BENCHMARK ****\n",
    "    \n",
    "#     # Basic Benchmark\n",
    "#     # Step 1. Find all MS with distance > 5km. \n",
    "#     # Step 2. Sort by enrollment and select n schools with the highest enrollment\n",
    "#     # Step 3. Sum the enrollment to assume that the new schools will only serve n schoools.\n",
    "#     # Step 4. Save the results to results.csv\n",
    "\n",
    "#     start_time_overall = time.time()\n",
    "\n",
    "#     # Run Basic Benchmark Per Region\n",
    "#     for i in data:\n",
    "#         df_ms = data[i][0] # MS in region i\n",
    "#         ee_old_constant = np.sum(data[i][3]) # total SS_enrolment in region i\n",
    "#         A\n",
    "#         if (i == 'Addis Ababa') | (i == 'Dire Dawa'):  \n",
    "#             # Distances to secondary schools are lower in city administrations\n",
    "#             dd = df_ms[df_ms['nearest_lwr_sec'] > 2] # Find all MS > 2km distance in city administrations\n",
    "#         else:\n",
    "#             dd = df_ms[df_ms['nearest_lwr_sec'] > 5] # Find all MS > 5km distance in regions.\n",
    "\n",
    "#         dd = dd.sort_values(['grade_7_8'], ascending=False).head(n) # filter by number of SS to construct\n",
    "#         benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "#         benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "#         benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "        \n",
    "#         create_results_table() # create table for results, if it doesn't already exist.\n",
    "#         results = pd.read_csv('results.csv')\n",
    "#         row1 = (pd.Series({'region': i, 'proposed_schools_n': n,\n",
    "#                        'starting_point':np.nan, 'algorithm':'Basic Benchmark', \n",
    "#                        'ee': abs(ee_old_constant - abs(benchmark)), 'eei': benchmark, \n",
    "#                        'proposed_locations': benchmark_loc, 'time':0, 'sigma':np.nan}))\n",
    "    \n",
    "#         results = results.append(row1, ignore_index=True)\n",
    "#         results.to_csv('results.csv', index=False)\n",
    "        \n",
    "#     print('Basic Algorithm Time:', n, time.time() - start_time_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all eligible region names. \n",
    "regions = set(df['ADM1_EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Algorithm Time: 5 0.08278942108154297\n",
      "Basic Algorithm Time: 10 0.07680726051330566\n",
      "Basic Algorithm Time: 5 0.08205127716064453\n",
      "Basic Algorithm Time: 10 0.07596230506896973\n",
      "Basic Algorithm Time: 5 0.10330867767333984\n",
      "Basic Algorithm Time: 10 0.1076042652130127\n",
      "Basic Algorithm Time: 5 0.1159677505493164\n",
      "Basic Algorithm Time: 10 0.1170186996459961\n",
      "Basic Algorithm Time: 5 0.1162712574005127\n",
      "Basic Algorithm Time: 10 0.10314393043518066\n",
      "Basic Algorithm Time: 5 0.11569523811340332\n",
      "Basic Algorithm Time: 10 0.10844922065734863\n",
      "Basic Algorithm Time: 5 0.12757301330566406\n",
      "Basic Algorithm Time: 10 0.11543416976928711\n",
      "Basic Algorithm Time: 5 0.11879348754882812\n",
      "Basic Algorithm Time: 10 0.11012792587280273\n",
      "Basic Algorithm Time: 5 0.12350320816040039\n",
      "Basic Algorithm Time: 10 0.12184524536132812\n"
     ]
    }
   ],
   "source": [
    "# Run Basic Benchmark and save results.\n",
    "for i in regions:\n",
    "    benchmarks(i, datasets, 5)\n",
    "    benchmarks(i, datasets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduced_search_space(data):\n",
    "#     dist = haversine_vector(data[2],data[2], Unit.KILOMETERS, comb=True) # distance of MS to x. \n",
    "#     # filter dataset if MS enrolment is above 200, or\n",
    "#     min_distance = dist < 5 # schools within 5km of each other\n",
    "#     find_enrolment = np.where(min_distance, data[1],0)\n",
    "#     min_capacity = np.sum(find_enrolment, axis=0) > 200 #set min capacity at 200.\n",
    "#     reduced_dataset = data[0][min_capacity].reset_index(drop=True)\n",
    "#     return reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable filtering of dataset based on max distance for inclusion. If >5km, only MS 5km from SS are considered. \n",
    "\n",
    "### TO DO: CREATE A FUNCTION THAT DOES THIS. \n",
    "\n",
    "min_dist_dict = { \n",
    "'Addis Ababa': 2, # Insufficient only 3 >5km: X, 9 MS > 3km: X, 21 > 2km: ✓\n",
    " 'Amhara': 5, # 3376 MS > 5km: ✓\n",
    " 'Benishangul Gumz': 5, # 204 MS > 5km: ✓\n",
    " 'Dire Dawa': 5, # 29 MS > 5km: ✓\n",
    " 'Harari': 3, # Insufficient only 10 MS >5km: X 21 > 3km: ✓\n",
    " 'Oromia': 5, # 5037 MS > 5km: ✓\n",
    " 'SNNP': 5, # 675 MS > 5km: ✓\n",
    " 'Somali': 5, # 272 MS > 5km: ✓\n",
    " 'Tigray': 5 # 686 schools > 5km: ✓\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for CMA. \n",
    "def find_eligible_MS(data, expected_schools, min_dict):\n",
    "    \n",
    "    # Step 1. Check is distance is > variable minimum distance by region outlined above.\n",
    "    data = data[data['nearest_lwr_sec'] > data['ADM1_EN'].map(min_dict)]\n",
    "    \n",
    "    # Step 2. Prepare key MS datasets. \n",
    "    df_middle = data.loc[data['grade_7_8'] > 0].reset_index(drop=True) # Filter to MS only\n",
    "    df_middle_enroll = df_middle['grade_7_8'].reset_index(drop=True).to_numpy(dtype=float) # Convert MS Enrolment to np\n",
    "    df_middle_loc = df_middle['point'].reset_index(drop=True).to_numpy()\n",
    "    df_middle_loc = np.array([np.array(i) for i in df_middle_loc], dtype=float) # Convert MS locational data to np.\n",
    "    \n",
    "    # Step 3. Calculate distance from MS to all other MS and check if < 5km\n",
    "    dist = haversine_vector(df_middle_loc,df_middle_loc, Unit.KILOMETERS, comb=True) < 5 # distance of MS < 5km\n",
    "    \n",
    "    # Step 4. Find MS enrolment if within 5km\n",
    "    find_enrolment = np.where(dist, df_middle_enroll,0) \n",
    "    new_enroll_ss = find_enrolment # create a copy for manipulation\n",
    "    \n",
    "    # Step 5. Expected new SS must be < length of dist. e.g. If length_dist = 15, new SS must be less than 15.\n",
    "    length_dist = len(np.sum(new_enroll_ss, axis=1)) \n",
    "    \n",
    "    # Step 6. Iteratively find top n enrolment of schools within 5km of each other. n=expected_schools or length_dist\n",
    "    # To ensure there is no double counting, find  first max, then subtract from array of new_enroll_ss.\n",
    "    # Repeat until expected enrolment found for top n schools. \n",
    "\n",
    "    for i in range(min(expected_schools, length_dist)):\n",
    "        expected_enrol = np.sum(new_enroll_ss, axis=1) \n",
    "        max_ee_index = np.argpartition(expected_enrol, -(i+1))[-(i+1):][-(i+1)]\n",
    "        max_array = new_enroll_ss[max_ee_index]\n",
    "        new_enroll_ss = (new_enroll_ss - new_enroll_ss[max_ee_index])\n",
    "        new_enroll_ss = new_enroll_ss.clip(min=0)\n",
    "        new_enroll_ss[max_ee_index] = max_array\n",
    "        \n",
    "    # Step 7. Find new schools above minimum capacity of 200.\n",
    "    new_enroll_ss = new_enroll_ss[np.sum(new_enroll_ss, axis=1) > 200] # minimum capacity of 200\n",
    "    max_possible = min(len(new_enroll_ss), expected_schools) # total new SS possible\n",
    "    new_schools_ee = np.sum(new_enroll_ss, axis=1) #  total ee possible from new SS.\n",
    "    new_schools_index = np.argpartition(new_schools_ee, -(max_possible))[-(max_possible):]\n",
    "\n",
    "    # Step 8. Find index of all new schools included in the final sample & return MS dataframe with only these MS.\n",
    "    cluster_schools = np.where(new_enroll_ss[new_schools_index])[1] \n",
    "    cluster_schools = df_middle.loc[cluster_schools]\n",
    "    region = list(pd.unique(data['ADM1_EN']))[0]\n",
    "    return cluster_schools, region, max_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all eligible MS, then return joined datasets. \n",
    "n = 5\n",
    "dataframes = [find_eligible_MS(df[df['ADM1_EN']==i], n, min_dist_dict) for i in regions]\n",
    "listy = dict([ [i[1], i[2]] for i in dataframes]) # what's possible. \n",
    "revised_df = pd.concat([df[0] for df in dataframes], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the new datasets. \n",
    "datasets2 = {}\n",
    "for i in regions:\n",
    "    datasets2[i] = prepare_datasets(revised_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FITNESS FUNCTION\n",
    "\n",
    "def expected_enrollment(middle_loc, x, middle_enroll, current_dist, ee_old_constant):\n",
    "\n",
    "    \"\"\" The Fitness Function returns the overall total expected enrollment increase given the new school locations x. \n",
    "\n",
    "        The function calculates the distance between each new school in x and the current middle schools to determine \n",
    "        the expected enrollment per new school. If a new school proposed is closer than the current secondary school, \n",
    "        enrollment from that subtracted from the overall enrollment gains. \n",
    "\n",
    "        Parameters:\n",
    "            middle_loc: a vector of longitude and latitude coordinates of all middle schools\n",
    "            x: a vector longitude and latitude coordinates for the newly proposed secondary schools\n",
    "            middle_enroll: a vector of current enrollment at each middle school\n",
    "            current_distnce: a vector of the current distances between each middle school and secondary school.\n",
    "\n",
    "        Returns:\n",
    "            eei + ee_old (float): the expected enrollment increase from each school in x  (eei)\n",
    "                                  + the current secondary enrollment (ee_old)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ee_old = ee_old_constant.copy() # Overall SS Enrollment\n",
    "    d = haversine_vector(middle_loc,x, Unit.KILOMETERS, comb=True) # distance of MS to x.\n",
    "    d_min = np.min(d, axis=0) # select only closest schools to avoid duplication. \n",
    "\n",
    "    d_index = np.argmin(d, axis=0) # index of min distance of MS to x\n",
    "    d2 = np.where((d_min <5) & (d_min < current_dist)) # limit to only schools < 5km and schools < current distance\n",
    "    # Put into dataframe for manipulation in pandas\n",
    "    # index 0 = x, index 1 = MS, index 2 = distance, index 3 = shaped enrollment \n",
    "    d3 = pd.DataFrame(np.vstack((d_index[d2], d2[0], d_min[d2], shape(d_min[d2], middle_enroll[d2[0]]))).T) \n",
    "    d32 = d3.loc[d3.groupby([1])[2].idxmin()] # find only nearby SS if MS is close to more than 1 SS.\n",
    "    d32 = d32.groupby(0)[3].sum() # The overall shaped enrollment for each school in x.\n",
    "    eei = np.sum(d32) # overall expected enrollment increase for each school in x.\n",
    "    # Find MS enrollment of MS if close to old SS. \n",
    "    distance_current = np.sum(shape(current_dist[d2], middle_enroll[d2]))\n",
    "    ee_old -= distance_current # subtract shaped enrollment from overall SS enrollment as new school is close\n",
    "    return eei + ee_old # return overall expected enrollment + current secondary enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x, n, ms_loc, ms_enrol, ms_dist, ee_old):\n",
    "    \"\"\" The Objective Function returns the maximum expected enrollment.\n",
    "    \"\"\"\n",
    "    x = x.reshape(n,2) # reshape for input into expected enrolment.\n",
    "    t_case = expected_enrollment(ms_loc, x, ms_enrol, ms_dist, ee_old) # run fitness function.\n",
    "    return t_case*-1 # Multiply by -1 for maximising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(datasets['Tigray'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df['grade9_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_benchmarks(data, n, min_dict):\n",
    "    # *** ESTABLISH BENCHMARK ****\n",
    "    \n",
    "    # Basic Benchmark\n",
    "    # Step 1. Find all MS with distance > 5km. \n",
    "    # Step 2. Sort by enrollment and select n schools with the highest enrollment\n",
    "    # Step 3. Sum the enrollment to assume that the new schools will only serve n schoools.\n",
    "    # Step 4. Save the results to results.csv\n",
    "    start_time_basic = time.time()\n",
    "    df_ms, df_ms_enrol, df_ms_loc, current_ms_dist = data[0], data[1], data[2], data[5]\n",
    "    region = df_ms['ADM1_EN'].unique()[0]\n",
    "    ee_old_constant = np.sum(data[3])\n",
    "    df_ms = df_ms[df_ms['nearest_lwr_sec'] > df_ms['ADM1_EN'].map(min_dict)]\n",
    "    dd = df_ms.sort_values(['grade_7_8'], ascending=False).head(n) # filter by number of SS to construct\n",
    "    n = len(dd)\n",
    "    benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "    basic_time = time.time() - start_time_basic\n",
    "\n",
    "    # Advanced Benchmark\n",
    "    # Since MS with the highest enrollment are likely to be close to other MS, apply the fitness function to these schools\n",
    "    # Step 1. Assume x is the locations of the top n schools\n",
    "    # Step 2. Run fitness Function on these schools. \n",
    "    start_time_adv = time.time()\n",
    "    benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "    benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "    \n",
    "    # Run Benchmark on objecitve function. \n",
    "    benchmark_f = f2(benchmark_loc, n, df_ms_loc, df_ms_enrol, current_ms_dist, ee_old_constant) # Run Fitness Function\n",
    "    # Store results\n",
    "    create_results_table() # create table for results, if it doesn't already exist.\n",
    "    results = pd.read_csv('results.csv')\n",
    "\n",
    "    row1 = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                       'starting_point':np.nan, 'algorithm':'Basic Benchmark', \n",
    "                       'ee': benchmark + ee_old_constant, 'eei': benchmark,\n",
    "                       'proposed_locations': benchmark_loc, 'time':np.nan, 'sigma':np.nan}))\n",
    "\n",
    "    row2 = (pd.Series({'region': region, 'proposed_schools_n': n,'starting_point': np.nan, \n",
    "                       'algorithm':'Advanced Benchmark', 'ee': abs(round(benchmark_f,0)),\n",
    "                       'eei': round(abs(benchmark_f)-ee_old_constant,0), \n",
    "                        'proposed_locations': benchmark_loc, 'time':np.nan, 'sigma':np.nan}))\n",
    "\n",
    "    results = results.append([row1,row2], ignore_index=True)\n",
    "    results.to_csv('results.csv', index=False)\n",
    "\n",
    "    print(region, 'n=', n, ', Basic Algorithm Time:', basic_time)\n",
    "    print(region, 'n=', n, ', ', 'Advanced Algorithm Time:', time.time() - start_time_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "for i in regions:\n",
    "    establish_benchmarks(datasets[i], n, min_dist_dict)\n",
    "\n",
    "n = 10\n",
    "for i in regions:\n",
    "    establish_benchmarks(datasets[i], n, min_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(f, max_iterations, sp, n):\n",
    "    x = [generate_sp_proposed(sp, n) for _ in range(max_iterations)]\n",
    "    fx = [[f(xi), xi] for xi in x]\n",
    "    best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "    return best_f, best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO SHARE THE random_sp per region. \n",
    "def create_random_sps(region, data_bounds):\n",
    "    bounds = get_bounds(data_bounds)\n",
    "    sp = generate_random_sp(bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_random_sps('Amhara', datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(n, n_sp, max_iterations, data):\n",
    "    start_time_rs = time.time()\n",
    "    df_ms, df_ms_enrol, df_ms_loc, current_ms_dist = data[0], data[1], data[2], data[5]\n",
    "    bounds = get_bounds(data[0])\n",
    "    sp = generate_random_sp(bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "    ee_old_constant = np.sum(data[3])\n",
    "    print('done here')\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\" The Objective Function for Random Search.\n",
    "        \"\"\"\n",
    "        x = x.reshape(n,2) # reshape for input into expected enrolment.\n",
    "        test_case = expected_enrollment(df_ms_loc, x, df_ms_enrol, current_ms_dist, ee_old_constant) # run fitness function.\n",
    "        return test_case*-1 # Multiply by -1 for maximising.\n",
    "    \n",
    "    fx = []\n",
    "    for _ in range(n_sp):\n",
    "        start_time = time.time()\n",
    "        fx.append([random_search(f, max_iterations, sp, n), time.time() - start_time])\n",
    "    \n",
    "    results = pd.read_csv('results.csv')\n",
    "    for i in range(0, len(fx)):\n",
    "        row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                        'starting_point':i, 'algorithm':'Random Search', 'ee':round(abs(fx[i][0][0]),0),\n",
    "                                    'eei':round(abs(ee_old_constant - abs(fx[i][0][0])),0), \n",
    "                                    'proposed_locations': fx[i][0][1], 'time':fx[i][1], 'sigma':'NA'}))\n",
    "        results = results.append(row, ignore_index=True)\n",
    "\n",
    "    print(region, 'RS complete. Total time:', round(time.time() - start_time_rs,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_random_search(5, 30, 1000, datasets['Harari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a large initial sample of starting points. \n",
    "    sp = generate_random_sp(lat_bounds, lon_bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce search space for CMA-ES. \n",
    "# datasets2 = {}\n",
    "# for i in datasets:\n",
    "#     datasets2[i] = prepare_datasets(reduced_search_space(datasets[i]), i)\n",
    "#     datasets2[i][3] = datasets[i][3]\n",
    "#     datasets2[i][4] = datasets[i][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'Oromia'\n",
    "gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']==region]['geometry'].reset_index(drop=True)\n",
    "n = listy[region]\n",
    "df_test1 = datasets[region]\n",
    "df_test2 = datasets2[region]\n",
    "df_ms_subset = df_test2[0]\n",
    "df_ms_enrol_subset = df_test2[1]\n",
    "df_ms_loc_subset = df_test2[2]\n",
    "current_ms_dist_subset = df_test2[5]\n",
    "df_ss_enrol = df_test1[3]\n",
    "df_ss_loc = df_test1[4]\n",
    "\n",
    "bounds = get_bounds(df_ms_subset)\n",
    "cma_bounds = get_cma_bounds(bounds, n)\n",
    "\n",
    "# Create a large initial sample of starting points. \n",
    "\n",
    "sp = generate_random_sp(bounds)# Create a large sample of starting points\n",
    "sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "ee_old_constant = np.sum(df_ss_enrol) # existing enrollment for secondary. The constant figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_old_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_ms_enrol_subset) # should be able to attain this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(f, iterations, rand_sp):\n",
    "    x = [rand_sp for _ in range(iterations)]\n",
    "    fx = [[f(xi), xi] for xi in x]\n",
    "    best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "    return best_f, best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     *** OBJECTIVE FUNCTION ****\n",
    "    \n",
    "def f(x):\n",
    "    \"\"\" The Objective Function returns the maximum expected enrollment.\n",
    "    \"\"\"\n",
    "    x = x.reshape(n,2) # reshape for input into expected enrolment.\n",
    "    test_case = expected_enrollment(df_ms_loc_subset, x, df_ms_enrol_subset, current_ms_dist_subset) # run fitness function.\n",
    "    return test_case*-1 # Multiply by -1 for maximising.\n",
    "\n",
    "dd = df_ms_subset.sort_values(['grade_7_8'], ascending=False).head(n)\n",
    "# print(dd)\n",
    "benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "# print(benchmark)\n",
    "benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "benchmark_f = f(benchmark_loc) # Run Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def random_search(f, iterations, rand_sp):\n",
    "        x = [rand_sp for _ in range(iterations)]\n",
    "        fx = [[f(xi), xi] for xi in x]\n",
    "        best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "        return best_f, best_solution\n",
    "    \n",
    "    start_time_rs = time.time()\n",
    "    fx = []\n",
    "    for _ in range(n_starting_points):\n",
    "        start_time = time.time()\n",
    "        fx.append([random_search(f, maxits), time.time() - start_time])\n",
    "        \n",
    "    for i in range(0, len(fx)):\n",
    "        row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                        'starting_point':i, 'algorithm':'Random Search', 'ee':round(abs(fx[i][0][0]),0),\n",
    "                                    'eei':round(abs(ee_old_constant - abs(fx[i][0][0])),0), \n",
    "                                    'proposed_locations': fx[i][0][1], 'time':fx[i][1], 'sigma':'NA'}))\n",
    "        results = results.append(row, ignore_index=True)\n",
    "        \n",
    "    print(region, 'RS complete. Total time:', round(time.time() - start_time_rs,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_starting_points = 30\n",
    "maxits = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sigma0 = np.sqrt(np.std(df_ms_loc_subset[:,0])**2 + np.std(df_ms_loc_subset[:,1])**2)\n",
    "sigmas = np.linspace(0, sigma0, 11)[1:] # Take 10 values evenly spaced out between 0 to sigma0\n",
    "\n",
    "start_time_cma = time.time()\n",
    "fcma = []\n",
    "for i in range(n_starting_points):\n",
    "        start_time = time.time()\n",
    "        for j in sigmas:\n",
    "            es = cma.CMAEvolutionStrategy(generate_sp_proposed(sp, n).flatten(), sigma0=j,\n",
    "                                      inopts={'bounds': cma_bounds,'seed':1234})\n",
    "            es.optimize(f, iterations=(maxits/ es.popsize))\n",
    "        fcma.append((es.result[1], es.result[0].reshape(n, 2), (time.time() - start_time), j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "626675.0588190556 -  625248.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1427/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_ms_enrol_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(abs(ee_old_constant - abs(fcma[0][0])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(3.570095, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(df, region, n):\n",
    "    \"\"\" Function to run all experiments per region according to the total new schools proposed. \n",
    "        Doesn't return any value. It appends the results dataframe and creates a set of visualisations per region.\n",
    "    \"\"\"\n",
    "    # limit geojson to only selected region\n",
    "    # limit clean dataset to only selected region\n",
    "    gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "    gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']==region]['geometry'].reset_index(drop=True)\n",
    "    df = df.loc[df['ADM1_EN'] == region]\n",
    "    \n",
    "    # Find regional boundaries in lat lon.\n",
    "    # Latitude is the Y axis, longitude is the X axis.\n",
    "    bounds = gdf_region_shp.bounds \n",
    "    lat_bounds = bounds[['miny','maxy']].to_numpy(dtype=float)[0]\n",
    "    lon_bounds = bounds[['minx','maxx']].to_numpy(dtype=float)[0]\n",
    "    bounds = np.array([[lat_bounds[0], lon_bounds[0]], [lat_bounds[1], lon_bounds[1]]])\n",
    "    # array - [[lower lat bounds, lower lon bounds],[upper lat bounds, upper lon bounds]]\n",
    "    \n",
    "    # CMA expects a list of size 2 for bounds\n",
    "    x1y1 = np.repeat([bounds[0,:]],n, axis=0).flatten()\n",
    "    x2y2 = np.repeat([bounds[1,:]],n, axis=0).flatten()\n",
    "    boundsxy = [x1y1,x2y2]\n",
    "    \n",
    "    # Create subset arrays in numpy for input to the Fitness Function.\n",
    "    # 1. df_middle_enroll: MS enrollment data. Only the last two grades used as predictors for expected SS enrollment\n",
    "    # 2. df_middle_loc: MS location data- lat lon point data. \n",
    "    \n",
    "    # 3. df_sec_enroll: SS enrollment data. Only grades 9 and 10 enrollment.\n",
    "    # 4. df_sec_loc: SS location data- lat lon point data. \n",
    "    # 5. current_ms_distance: existing distances from MS to SS\n",
    "\n",
    "    df_middle = df.loc[df['grade_7_8'] > 0]\n",
    "    df_middle_enroll = df_middle['grade_7_8'].reset_index(drop=True).to_numpy(dtype=float) # 1\n",
    "    df_middle_loc = df_middle['point'].reset_index(drop=True).to_numpy()\n",
    "    df_middle_loc = np.array([np.array(i) for i in df_middle_loc], dtype=float) # 2\n",
    "\n",
    "    df_sec = df.loc[ (df['gr_offer'] == 'G. 9-10') | (df['gr_offer'] == 'G. 9-12')]\n",
    "    df_sec_enroll = df_sec['grade9_10'].reset_index(drop=True).to_numpy(dtype=float)\n",
    "    df_sec_loc = df_sec['point'].reset_index(drop=True).to_numpy() \n",
    "    df_sec_loc = np.array([np.array(i) for i in df_sec_loc], dtype=float) # 3\n",
    "    current_ms_distance = df_middle['nearest_lwr_sec'].to_numpy() # 5\n",
    "    \n",
    "    # Create a large initial sample of starting points. \n",
    "    sp = generate_random_sp(lat_bounds, lon_bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "    \n",
    "    ee_old_constant = np.sum(df_sec_enroll) # existing enrollment for secondary. The constant figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[df['grade_7_8'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_clusters >5km away, then identify best location in these priority areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Filter by recommended distance per region\n",
    "# Step 2. Calculate distance from MS to all other MS. \n",
    "# Step 3. Find Enrolment of MS within 5km (self inclusive)\n",
    "# Step 4. Return feeder MS with a minimum capacity of 200 students. \n",
    "# Step 5. Find the index of MS with highest feeder enrolment. \n",
    "# Step 6. Return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_search_space3(df[df['ADM1_EN']=='Oromia'], 10,m, min_dist_to_sec, min_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.array([5,2,3,8])\n",
    "np.argpartition(tt, -3)[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about existing secondary?\n",
    "\n",
    "def reduced_search_space2(df, n, m):\n",
    "    # filter all \n",
    "    data = data[0][data[0]['nearest_lwr_sec'] > 5]\n",
    "\n",
    "#     dist = haversine_vector(data[2],data[2], Unit.KILOMETERS, comb=True) # distance of MS to x. \n",
    "#     # filter dataset if MS enrolment is above 200, or\n",
    "#     min_distance = dist < 5 # schools within 5km of each other\n",
    "#     # should remove schools < 3km. \n",
    "#     find_enrolment = np.where(min_distance, data[1],0)\n",
    "#     min_capacity = np.sum(find_enrolment, axis=0) > 200 #set min capacity at 200.\n",
    "#     # sort instead of choose?\n",
    "#     max20 = np.argpartition(min_capacity, -(n+m))[-(n+m):] # best 20 schools MUST have nearest_lwr_sec > 5 EDIT TOMORROW!\n",
    "#     # then filter by those > 5km?\n",
    "#     demand_points = np.argwhere(find_enrolment[max20])[:,1] # index of max20\n",
    "#     reduced_dataset = data[0].iloc[demand_points].reset_index(drop=True)\n",
    "#     return reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['nearest_lwr_sec'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = reduced_search_space2(datasets['Oromia'], 5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rr['nearest_lwr_sec'] > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = datasets['Amhara']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy[0]['nearest_lwr_sec'] > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr['nearest_lwr_sec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce search space for CMA-ES. \n",
    "datasets2 = {}\n",
    "for i in datasets:\n",
    "    datasets2[i] = prepare_datasets(reduced_search_space(datasets[i]), i)\n",
    "    datasets2[i][3] = datasets[i][3]\n",
    "    datasets2[i][4] = datasets[i][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = get_bounds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1y1 = np.repeat(bounds[0,:],2)\n",
    "x2y2 = np.repeat(bounds[1,:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = Polygon(zip(np.repeat(bounds[:, 0],2),np.repeat(bounds[:, 1],2)))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = 'epsg:4326'\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[poly])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(lon_list, lat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x1y1 = np.repeat([bounds[0,:]],n, axis=0).flatten()\n",
    "    x2y2 = np.repeat([bounds[1,:]],n, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area difference between old and new functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = gpd.GeoSeries(bounds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara = datasets['Amhara']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset if MS enrolment is above 200, or\n",
    "dist = haversine_vector(amhara[2],amhara[2], Unit.KILOMETERS, comb=True) # distance of MS to x. \n",
    "# dist = remove_diagonals(dist)\n",
    "dist5 = dist < 5\n",
    "enrol5 = np.where(dist5, amhara[1],0)\n",
    "enrol = np.sum(enrol5, axis=0) > 200\n",
    "# amhara[0].reset_index(drop=True)[enrol]\n",
    "\n",
    "max20 = np.argpartition(enrol, -20)[-20:]\n",
    "demand_points = np.argwhere(enrol5[max20])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(enrol5[max20][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara[0].reset_index(drop=True).loc[max20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara[0].reset_index(drop=True)[max20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara[0][max20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(enrol5, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.any(enrol5t, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(amhara[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrol5t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(amhara[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(amhara[0].reset_index(drop=True)[enrol5t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara[3][np.where(dist5[0], amhara[3],0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dist5:\n",
    "    np.where( i == True, )\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dist5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.sum(dist5[amhara[3].astype(int)], axis=0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dist5:\n",
    "    i[amhara[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist5[np.array(amhara[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dist:\n",
    "    print(i[dist5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(dist <5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dist <5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist5[amhara[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substantially reduce the search space. \n",
    "np.where(dist[dist5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist[dist5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[within5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(within5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_index = np.argmin(dist2, axis=0) # index of min distance of MS to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the most expensive component is CMA. Modularity of programmes is geared arounds this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of dataset\n",
    "# include only schools that are above a minimum enrolment of 200, or accumulation of MS_Enroll within 5km > 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes self-comparisons.\n",
    "def remove_diagonals(xx):\n",
    "    # Remove diagonals as it is the distance to itself. \n",
    "    # Source: https://pyquestions.com/deleting-diagonal-elements-of-a-numpy-array\n",
    "    xx = xx[~np.eye(xx.shape[0], dtype=bool)].reshape(xx.shape[0],-1)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara = prepare_datasets(df, 'Amhara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original Function. \n",
    "\n",
    "\n",
    "def run(df, region, n):\n",
    "    \"\"\" Function to run all experiments per region according to the total new schools proposed. \n",
    "        Doesn't return any value. It appends the results dataframe and creates a set of visualisations per region.\n",
    "    \"\"\"\n",
    "    # limit geojson to only selected region\n",
    "    # limit clean dataset to only selected region\n",
    "    gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "    gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']==region]['geometry'].reset_index(drop=True)\n",
    "    df = df.loc[df['ADM1_EN'] == region]\n",
    "    \n",
    "    # Find regional boundaries in lat lon.\n",
    "    # Latitude is the Y axis, longitude is the X axis.\n",
    "    bounds = gdf_region_shp.bounds \n",
    "    lat_bounds = bounds[['miny','maxy']].to_numpy(dtype=float)[0]\n",
    "    lon_bounds = bounds[['minx','maxx']].to_numpy(dtype=float)[0]\n",
    "    bounds = np.array([[lat_bounds[0], lon_bounds[0]], [lat_bounds[1], lon_bounds[1]]])\n",
    "    # array - [[lower lat bounds, lower lon bounds],[upper lat bounds, upper lon bounds]]\n",
    "    \n",
    "    # CMA expects a list of size 2 for bounds\n",
    "    x1y1 = np.repeat([bounds[0,:]],n, axis=0).flatten()\n",
    "    x2y2 = np.repeat([bounds[1,:]],n, axis=0).flatten()\n",
    "    boundsxy = [x1y1,x2y2]\n",
    "    \n",
    "    # Create subset arrays in numpy for input to the Fitness Function.\n",
    "    # 1. df_middle_enroll: MS enrollment data. Only the last two grades used as predictors for expected SS enrollment\n",
    "    # 2. df_middle_loc: MS location data- lat lon point data. \n",
    "    \n",
    "    # 3. df_sec_enroll: SS enrollment data. Only grades 9 and 10 enrollment.\n",
    "    # 4. df_sec_loc: SS location data- lat lon point data. \n",
    "    # 5. current_ms_distance: existing distances from MS to SS\n",
    "\n",
    "    df_middle = df.loc[df['grade_7_8'] > 0]\n",
    "    df_middle_enroll = df_middle['grade_7_8'].reset_index(drop=True).to_numpy(dtype=float) # 1\n",
    "    df_middle_loc = df_middle['point'].reset_index(drop=True).to_numpy()\n",
    "    df_middle_loc = np.array([np.array(i) for i in df_middle_loc], dtype=float) # 2\n",
    "\n",
    "    df_sec = df.loc[ (df['gr_offer'] == 'G. 9-10') | (df['gr_offer'] == 'G. 9-12')]\n",
    "    df_sec_enroll = df_sec['grade9_10'].reset_index(drop=True).to_numpy(dtype=float)\n",
    "    df_sec_loc = df_sec['point'].reset_index(drop=True).to_numpy() \n",
    "    df_sec_loc = np.array([np.array(i) for i in df_sec_loc], dtype=float) # 3\n",
    "    current_ms_distance = df_middle['nearest_lwr_sec'].to_numpy() # 5\n",
    "    \n",
    "    # Create a large initial sample of starting points. \n",
    "    sp = generate_random_sp(lat_bounds, lon_bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "    \n",
    "    ee_old_constant = np.sum(df_sec_enroll) # existing enrollment for secondary. The constant figure. \n",
    "    \n",
    "#     best_sp = df_middle.nlargest(n, 'grade_7_8')['point'].reset_index(drop=True).to_numpy() \n",
    "#     best_sp = np.array([np.array(i) for i in best_sp], dtype=float) # 3\n",
    "\n",
    "#     *** FITNESS FUNCTION ****\n",
    "\n",
    "    def expected_enrollment(middle_loc, x, middle_enroll, current_dist):\n",
    "        \n",
    "        \"\"\" The Fitness Function returns the overall total expected enrollment increase given the new school locations x. \n",
    "        \n",
    "            The function calculates the distance between each new school in x and the current middle schools to determine \n",
    "            the expected enrollment per new school. If a new school proposed is closer than the current secondary school, \n",
    "            enrollment from that subtracted from the overall enrollment gains. \n",
    "        \n",
    "            Parameters:\n",
    "                middle_loc: a vector of longitude and latitude coordinates of all middle schools\n",
    "                x: a vector longitude and latitude coordinates for the newly proposed secondary schools\n",
    "                middle_enroll: a vector of current enrollment at each middle school\n",
    "                current_distnce: a vector of the current distances between each middle school and secondary school.\n",
    "                            \n",
    "            Returns:\n",
    "                eei + ee_old (float): the expected enrollment increase from each school in x  (eei)\n",
    "                                      + the current secondary enrollment (ee_old)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ee_old = ee_old_constant.copy() # Overall SS Enrollment\n",
    "        d = haversine_vector(middle_loc,x, Unit.KILOMETERS, comb=True) # distance of MS to x. \n",
    "        d_min = np.min(d, axis=0) # select only closest schools to avoid duplication. \n",
    "        \n",
    "        d_index = np.argmin(d, axis=0) # index of min distance of MS to x\n",
    "        d2 = np.where((d_min <5) & (d_min < current_dist)) # limit to only schools < 5km and schools < current distance\n",
    "        # Put into dataframe for manipulation in pandas\n",
    "        # index 0 = x, index 1 = MS, index 2 = distance, index 3 = shaped enrollment \n",
    "        d3 = pd.DataFrame(np.vstack((d_index[d2], d2[0], d_min[d2], shape(d_min[d2], middle_enroll[d2[0]]))).T) \n",
    "        d32 = d3.loc[d3.groupby([1])[2].idxmin()] # find only nearby SS if MS is close to more than 1 SS.\n",
    "        d32 = d32.groupby(0)[3].sum() # The overall shaped enrollment for each school in x.\n",
    "        eei = np.sum(d32) # overall expected enrollment increase for each school in x.\n",
    "        # Find MS enrollment of MS if close to old SS. \n",
    "        distance_current = np.sum(shape(current_dist[d2], middle_enroll[d2]))\n",
    "        ee_old -= distance_current # subtract shaped enrollment from overall SS enrollment as new school is closer\n",
    "        return eei + ee_old # return overall expected enrollment + current secondary enrollment\n",
    "\n",
    "#     *** OBJECTIVE FUNCTION ****\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\" The Objective Function returns the maximum expected enrollment.\n",
    "        \"\"\"\n",
    "        x = x.reshape(n,2) # reshape for input into expected enrolment.\n",
    "        test_case = expected_enrollment(df_middle_loc, x, df_middle_enroll, current_ms_distance) # run fitness function.\n",
    "        return test_case*-1 # Multiply by -1 for maximising.\n",
    "    \n",
    "    \n",
    "    # **** RESULTS ****\n",
    "    \n",
    "    create_results_table() # create table for results, if it doesn't already exist.\n",
    "    results = pd.read_csv('results.csv')\n",
    "    \n",
    "    # *** ESTABLISH BENCHMARK ****\n",
    "    \n",
    "    # Basic Benchmark\n",
    "    # Step 1. Find all MS with distance > 5km. \n",
    "    # Step 2. Sort by enrollment and select n schools with the highest enrollment\n",
    "    # Step 3. Sum the enrollment to assume that the new schools will only serve n schoools.\n",
    "    # This is the basic benchmark\n",
    "    \n",
    "    if (region == 'Addis Ababa') | (region == 'Dire Dawa'):  \n",
    "        # Distances to secondary schools are lower in city administrations\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 2] # Find all PS > 2km distance in city administrations\n",
    "    else:\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 5] # Find all PS > 5km distance in regions.\n",
    "    dd = dd.sort_values(['grade_7_8'], ascending=False).head(n) # filter by number of SS to construct\n",
    "    benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "\n",
    "    # Advanced Benchmark\n",
    "    # Since MS with the highest enrollment are likely to be close to other MS, apply the fitness function to these schools\n",
    "    # Step 1. Assume x is the locations of the top n schools\n",
    "    # Step 2. Run fitness Function on these schools. \n",
    "    start_time = time.time()\n",
    "    benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "    benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "    benchmark_f = f(benchmark_loc) # Run Fitness Function\n",
    "       \n",
    "    # Store results\n",
    "    \n",
    "    row1 = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                       'starting_point':np.nan, 'algorithm':'Basic Benchmark', \n",
    "                       'ee': abs(ee_old_constant - abs(benchmark)), 'eei': benchmark, \n",
    "                       'proposed_locations': benchmark_loc, 'time':0, 'sigma':np.nan}))\n",
    "    \n",
    "    row2 = (pd.Series({'region': region, 'proposed_schools_n': n,'starting_point': np.nan, \n",
    "                       'algorithm':'Advanced Benchmark', 'ee': round(abs(benchmark_f)),\n",
    "                        'eei': round(abs(ee_old_constant  - abs(benchmark_f)),0), \n",
    "                        'proposed_locations': benchmark_loc, 'time':(time.time() - start_time), 'sigma':np.nan}))\n",
    "\n",
    "    results = results.append(row1, ignore_index=True)\n",
    "    results = results.append(row2, ignore_index=True)\n",
    "    \n",
    "    # Random Search Algorothm is used to see if an improved solution can be identified.\n",
    "    \n",
    "    # Set Parameters for experiments.\n",
    "    n_starting_points = 30\n",
    "    maxits = 20000 # max iterations\n",
    "    \n",
    "    best_sigma = {'Addis Ababa': 0.098445155,\n",
    "                 'Amhara': 0.86193024,\n",
    "                 'Benishangul Gumz': 0.443843224,\n",
    "                 'Dire Dawa': 0.15484862300000002,\n",
    "                 'Harari': 0.08544453699999999,\n",
    "                 'Oromia': 2.018435726,\n",
    "                 'SNNP': 0.14630103,\n",
    "                 'Somali': 0.34336816299999995,\n",
    "                 'Tigray': 0.555333321}\n",
    "    \n",
    "#     def random_search(f, iterations):\n",
    "#         x = [generate_sp_proposed(sp, n) for _ in range(iterations)]\n",
    "#         fx = [[f(xi), xi] for xi in x]\n",
    "#         best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "#         return best_f, best_solution\n",
    "    \n",
    "#     start_time_rs = time.time()\n",
    "#     fx = []\n",
    "#     for _ in range(n_starting_points):\n",
    "#         start_time = time.time()\n",
    "#         fx.append([random_search(f, maxits), time.time() - start_time])\n",
    "        \n",
    "#     for i in range(0, len(fx)):\n",
    "#         row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "#                         'iteration':i, 'algorithm':'Random Search', 'ee':round(abs(fx[i][0][0]),0),\n",
    "#                                     'eei':round(abs(ee_old_constant - abs(fx[i][0][0])),0), \n",
    "#                                     'proposed_locations': fx[i][0][1], 'time':fx[i][1], 'sigma':'NA'}))\n",
    "#         results = results.append(row, ignore_index=True)\n",
    "        \n",
    "#     print(region, 'RS complete. Total time:', round(time.time() - start_time_rs,2))\n",
    "    \n",
    "    # Covariance Matrix Adaptation-  CMA\n",
    "    \n",
    "    # Sigma is the initial standard deviation. Problem variables need to be scaled per region.\n",
    "    # Take 10 values between 0 and the variance of all MS\n",
    "#     sigma0 = 1.5*(np.sqrt(np.std(df_middle_loc[:,0])**2 + np.std(df_middle_loc[:,1])**2))\n",
    "#     sigmas = np.linspace(0, sigma0, 11)[1:] # Take 10 values evenly spaced out between 0 to sigma0\n",
    "    \n",
    "    start_time_cma = time.time()\n",
    "    fcma = []\n",
    "    for i in range(n_starting_points):\n",
    "        start_time = time.time()\n",
    "#         for j in sigmas:\n",
    "        es = cma.CMAEvolutionStrategy(generate_sp_proposed(sp, n).flatten(), sigma0=best_sigma[region],\n",
    "                                  inopts={'bounds': boundsxy,'seed':1234})\n",
    "        es.optimize(f, iterations=(maxits/ es.popsize))\n",
    "        fcma.append((es.result[1], es.result[0].reshape(n, 2), (time.time() - start_time), best_sigma[region]))\n",
    "            \n",
    "    for i in range(0, len(fcma)):\n",
    "        row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                    'starting_point':i, 'algorithm':'CMA', 'ee':round(abs(fcma[i][0]),0),\n",
    "                                'eei':round(abs(ee_old_constant - abs(fcma[i][0])),0), \n",
    "                                'proposed_locations': fcma[i][1], 'time':fcma[i][2], 'sigma':fcma[i][3]}))\n",
    "        results = results.append(row, ignore_index=True)\n",
    "    \n",
    "    print(region, 'CMA complete. Total time:', round(time.time() - start_time_cma,2))\n",
    "    \n",
    "    results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always globally converge?\n",
    "# if maximum enrolment \n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundy = get_bounds(df[df['ADM1_EN']=='Amhara'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spish = generate_random_sp(boundy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']=='Amhara']['geometry'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spish[check_region(spish, gdf_region_shp)] # only include points that are within regional boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = generate_sp_proposed(spish, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(df['lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = generate_random_sp(lat_bounds, lon_bounds)# Create a large sample of starting points\n",
    "sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bounds(df[df['ADM1_EN']=='Amhara'], 'Amhara', 'other', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bounds(df[df['ADM1_EN']=='Amhara'], 'Amhara', 'minmax', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     else:\n",
    "#         gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "#         gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']==region]['geometry'].reset_index(drop=True)\n",
    "\n",
    "#         # Find regional boundaries in lat lon.\n",
    "#         # Latitude is the Y axis, longitude is the X axis.\n",
    "#         bounds = gdf_region_shp.bounds \n",
    "#         lat_bounds = bounds[['miny','maxy']].to_numpy(dtype=float)[0]\n",
    "#         lon_bounds = bounds[['minx','maxx']].to_numpy(dtype=float)[0]\n",
    "#         bounds = np.array([[lat_bounds[0], lon_bounds[0]], [lat_bounds[1], lon_bounds[1]]])\n",
    "#         # array - [[lower lat bounds, lower lon bounds],[upper lat bounds, upper lon bounds]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara5 = prepare_datasets(df, 'Amhara', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhara5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "# Basic Benchmark for 5, 10, 20 schools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addis_ababa =\n",
    "amhara = \n",
    "oromia = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_benchmark():\n",
    "    # *** ESTABLISH BENCHMARK ****\n",
    "    \n",
    "    # Basic Benchmark\n",
    "    # Step 1. Find all MS with distance > 5km. \n",
    "    # Step 2. Sort by enrollment and select n schools with the highest enrollment\n",
    "    # Step 3. Sum the enrollment to assume that the new schools will only serve n schoools.\n",
    "    # This is the basic benchmark\n",
    "    \n",
    "    if (region == 'Addis Ababa') | (region == 'Dire Dawa'):  \n",
    "        # Distances to secondary schools are lower in city administrations\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 2] # Find all PS > 2km distance in city administrations\n",
    "    else:\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 5] # Find all PS > 5km distance in regions.\n",
    "    dd = dd.sort_values(['grade_7_8'], ascending=False).head(n) # filter by number of SS to construct\n",
    "    benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(df, region, n):\n",
    "    \"\"\" Function to run all experiments per region according to the total new schools proposed. \n",
    "        Doesn't return any value. It appends the results dataframe and creates a set of visualisations per region.\n",
    "    \"\"\"\n",
    "    # limit geojson to only selected region\n",
    "    # limit clean dataset to only selected region\n",
    "    gdf_region = gpd.read_file('eth_shape_files/json//eth_admin1v2.json') # read in geojson\n",
    "    gdf_region_shp = gdf_region.loc[gdf_region['ADM1_EN']==region]['geometry'].reset_index(drop=True)\n",
    "    df = df.loc[df['ADM1_EN'] == region]\n",
    "    \n",
    "    # Find regional boundaries in lat lon.\n",
    "    # Latitude is the Y axis, longitude is the X axis.\n",
    "    bounds = gdf_region_shp.bounds \n",
    "    lat_bounds = bounds[['miny','maxy']].to_numpy(dtype=float)[0]\n",
    "    lon_bounds = bounds[['minx','maxx']].to_numpy(dtype=float)[0]\n",
    "    bounds = np.array([[lat_bounds[0], lon_bounds[0]], [lat_bounds[1], lon_bounds[1]]])\n",
    "    # array - [[lower lat bounds, lower lon bounds],[upper lat bounds, upper lon bounds]]\n",
    "    \n",
    "    # CMA expects a list of size 2 for bounds\n",
    "    x1y1 = np.repeat([bounds[0,:]],n, axis=0).flatten()\n",
    "    x2y2 = np.repeat([bounds[1,:]],n, axis=0).flatten()\n",
    "    boundsxy = [x1y1,x2y2]\n",
    "    \n",
    "    # Create subset arrays in numpy for input to the Fitness Function.\n",
    "    # 1. df_middle_enroll: MS enrollment data. Only the last two grades used as predictors for expected SS enrollment\n",
    "    # 2. df_middle_loc: MS location data- lat lon point data. \n",
    "    # 3. df_sec_enroll: SS enrollment data. Only grades 9 and 10 enrollment.\n",
    "    # 4. df_sec_loc: SS location data- lat lon point data. \n",
    "    # 5. current_ms_distance: existing distances from MS to SS\n",
    "\n",
    "    df_middle = df.loc[df['grade_7_8'] > 0]\n",
    "    df_middle_enroll = df_middle['grade_7_8'].reset_index(drop=True).to_numpy(dtype=float) # 1\n",
    "    df_middle_loc = df_middle['point'].reset_index(drop=True).to_numpy()\n",
    "    df_middle_loc = np.array([np.array(i) for i in df_middle_loc], dtype=float) # 2\n",
    "\n",
    "    df_sec = df.loc[ (df['gr_offer'] == 'G. 9-10') | (df['gr_offer'] == 'G. 9-12')]\n",
    "    df_sec_enroll = df_sec['grade9_10'].reset_index(drop=True).to_numpy(dtype=float)\n",
    "    df_sec_loc = df_sec['point'].reset_index(drop=True).to_numpy() \n",
    "    df_sec_loc = np.array([np.array(i) for i in df_sec_loc], dtype=float) # 3\n",
    "    current_ms_distance = df_middle['nearest_lwr_sec'].to_numpy() # 5\n",
    "    \n",
    "    # Create a large initial sample of starting points. \n",
    "    sp = generate_random_sp(lat_bounds, lon_bounds)# Create a large sample of starting points\n",
    "    sp = sp[check_region(sp, gdf_region_shp)] # only include points that are within regional boundaries.\n",
    "    \n",
    "    ee_old_constant = np.sum(df_sec_enroll) # existing enrollment for secondary. The constant figure. \n",
    "    \n",
    "#     best_sp = df_middle.nlargest(n, 'grade_7_8')['point'].reset_index(drop=True).to_numpy() \n",
    "#     best_sp = np.array([np.array(i) for i in best_sp], dtype=float) # 3\n",
    "\n",
    "#     *** FITNESS FUNCTION ****\n",
    "\n",
    "    def expected_enrollment(middle_loc, x, middle_enroll, current_dist):\n",
    "        \n",
    "        \"\"\" The Fitness Function returns the overall total expected enrollment increase given the new school locations x. \n",
    "        \n",
    "            The function calculates the distance between each new school in x and the current middle schools to determine \n",
    "            the expected enrollment per new school. If a new school proposed is closer than the current secondary school, \n",
    "            enrollment from that subtracted from the overall enrollment gains. \n",
    "        \n",
    "            Parameters:\n",
    "                middle_loc: a vector of longitude and latitude coordinates of all middle schools\n",
    "                x: a vector longitude and latitude coordinates for the newly proposed secondary schools\n",
    "                middle_enroll: a vector of current enrollment at each middle school\n",
    "                current_distnce: a vector of the current distances between each middle school and secondary school.\n",
    "                            \n",
    "            Returns:\n",
    "                eei + ee_old (float): the expected enrollment increase from each school in x  (eei)\n",
    "                                      + the current secondary enrollment (ee_old)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ee_old = ee_old_constant.copy() # Overall SS Enrollment\n",
    "        d = haversine_vector(middle_loc,x, Unit.KILOMETERS, comb=True) # distance of MS to x. \n",
    "        d_min = np.min(d, axis=0) # select only closest schools to avoid duplication. \n",
    "        \n",
    "        d_index = np.argmin(d, axis=0) # index of min distance of MS to x\n",
    "        d2 = np.where((d_min <5) & (d_min < current_dist)) # limit to only schools < 5km and schools < current distance\n",
    "        # Put into dataframe for manipulation in pandas\n",
    "        # index 0 = x, index 1 = MS, index 2 = distance, index 3 = shaped enrollment \n",
    "        d3 = pd.DataFrame(np.vstack((d_index[d2], d2[0], d_min[d2], shape(d_min[d2], middle_enroll[d2[0]]))).T) \n",
    "        d32 = d3.loc[d3.groupby([1])[2].idxmin()] # find only nearby SS if MS is close to more than 1 SS.\n",
    "        d32 = d32.groupby(0)[3].sum() # The overall shaped enrollment for each school in x.\n",
    "        eei = np.sum(d32) # overall expected enrollment increase for each school in x.\n",
    "        # Find MS enrollment of MS if close to old SS. \n",
    "        distance_current = np.sum(shape(current_dist[d2], middle_enroll[d2]))\n",
    "        ee_old -= distance_current # subtract shaped enrollment from overall SS enrollment as new school is closer\n",
    "        return eei + ee_old # return overall expected enrollment + current secondary enrollment\n",
    "\n",
    "#     *** OBJECTIVE FUNCTION ****\n",
    "    \n",
    "    def f(x):\n",
    "        \"\"\" The Objective Function returns the maximum expected enrollment.\n",
    "        \"\"\"\n",
    "        x = x.reshape(n,2) # reshape for input into expected enrolment.\n",
    "        test_case = expected_enrollment(df_middle_loc, x, df_middle_enroll, current_ms_distance) # run fitness function.\n",
    "        return test_case*-1 # Multiply by -1 for maximising.\n",
    "    \n",
    "    \n",
    "    # **** RESULTS ****\n",
    "    \n",
    "    create_results_table() # create table for results, if it doesn't already exist.\n",
    "    results = pd.read_csv('results.csv')\n",
    "    \n",
    "    # *** ESTABLISH BENCHMARK ****\n",
    "    \n",
    "    # Basic Benchmark\n",
    "    # Step 1. Find all MS with distance > 5km. \n",
    "    # Step 2. Sort by enrollment and select n schools with the highest enrollment\n",
    "    # Step 3. Sum the enrollment to assume that the new schools will only serve n schoools.\n",
    "    # This is the basic benchmark\n",
    "    \n",
    "    if (region == 'Addis Ababa') | (region == 'Dire Dawa'):  \n",
    "        # Distances to secondary schools are lower in city administrations\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 2] # Find all PS > 2km distance in city administrations\n",
    "    else:\n",
    "        dd = df_middle[df_middle['nearest_lwr_sec'] > 5] # Find all PS > 5km distance in regions.\n",
    "    dd = dd.sort_values(['grade_7_8'], ascending=False).head(n) # filter by number of SS to construct\n",
    "    benchmark = sum(dd['grade_7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "\n",
    "    # Advanced Benchmark\n",
    "    # Since MS with the highest enrollment are likely to be close to other MS, apply the fitness function to these schools\n",
    "    # Step 1. Assume x is the locations of the top n schools\n",
    "    # Step 2. Run fitness Function on these schools. \n",
    "    start_time = time.time()\n",
    "    benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "    benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "    benchmark_f = f(benchmark_loc) # Run Fitness Function\n",
    "       \n",
    "    # Store results\n",
    "    \n",
    "    row1 = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                       'random_starting_point':np.nan, 'algorithm':'Basic Benchmark', \n",
    "                       'ee': abs(ee_old_constant - abs(benchmark)), 'eei': benchmark, \n",
    "                       'proposed_locations': benchmark_loc, 'time':0, 'sigma':np.nan}))\n",
    "    \n",
    "    row2 = (pd.Series({'region': region, 'proposed_schools_n': n,'iteration': np.nan, \n",
    "                       'algorithm':'Advanced Benchmark', 'ee': round(abs(benchmark_f)),\n",
    "                        'eei': round(abs(ee_old_constant  - abs(benchmark_f)),0), \n",
    "                        'proposed_locations': benchmark_loc, 'time':(time.time() - start_time), 'sigma':np.nan}))\n",
    "\n",
    "    results = results.append(row1, ignore_index=True)\n",
    "    results = results.append(row2, ignore_index=True)\n",
    "    \n",
    "    # Random Search Algorothm is used to see if an improved solution can be identified.\n",
    "    \n",
    "    # Set Parameters for experiments.\n",
    "    n_starting_points = 30\n",
    "    maxits = 20000 # max iterations\n",
    "    \n",
    "    best_sigma = {'Addis Ababa': 0.098445155,\n",
    "                 'Amhara': 0.86193024,\n",
    "                 'Benishangul Gumz': 0.443843224,\n",
    "                 'Dire Dawa': 0.15484862300000002,\n",
    "                 'Harari': 0.08544453699999999,\n",
    "                 'Oromia': 2.018435726,\n",
    "                 'SNNP': 0.14630103,\n",
    "                 'Somali': 0.34336816299999995,\n",
    "                 'Tigray': 0.555333321}\n",
    "    \n",
    "#     def random_search(f, iterations):\n",
    "#         x = [generate_sp_proposed(sp, n) for _ in range(iterations)]\n",
    "#         fx = [[f(xi), xi] for xi in x]\n",
    "#         best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "#         return best_f, best_solution\n",
    "    \n",
    "#     start_time_rs = time.time()\n",
    "#     fx = []\n",
    "#     for _ in range(n_starting_points):\n",
    "#         start_time = time.time()\n",
    "#         fx.append([random_search(f, maxits), time.time() - start_time])\n",
    "        \n",
    "#     for i in range(0, len(fx)):\n",
    "#         row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "#                         'iteration':i, 'algorithm':'Random Search', 'ee':round(abs(fx[i][0][0]),0),\n",
    "#                                     'eei':round(abs(ee_old_constant - abs(fx[i][0][0])),0), \n",
    "#                                     'proposed_locations': fx[i][0][1], 'time':fx[i][1], 'sigma':'NA'}))\n",
    "#         results = results.append(row, ignore_index=True)\n",
    "        \n",
    "#     print(region, 'RS complete. Total time:', round(time.time() - start_time_rs,2))\n",
    "    \n",
    "    # Covariance Matrix Adaptation-  CMA\n",
    "    \n",
    "    # Sigma is the initial standard deviation. Problem variables need to be scaled per region.\n",
    "    # Take 10 values between 0 and the variance of all MS\n",
    "#     sigma0 = 1.5*(np.sqrt(np.std(df_middle_loc[:,0])**2 + np.std(df_middle_loc[:,1])**2))\n",
    "#     sigmas = np.linspace(0, sigma0, 11)[1:] # Take 10 values evenly spaced out between 0 to sigma0\n",
    "    \n",
    "    start_time_cma = time.time()\n",
    "    fcma = []\n",
    "    for i in range(n_starting_points):\n",
    "        start_time = time.time()\n",
    "#         for j in sigmas:\n",
    "        es = cma.CMAEvolutionStrategy(generate_sp_proposed(sp, n).flatten(), sigma0=best_sigma[region],\n",
    "                                  inopts={'bounds': boundsxy,'seed':1234})\n",
    "        es.optimize(f, iterations=(maxits/ es.popsize))\n",
    "        fcma.append((es.result[1], es.result[0].reshape(n, 2), (time.time() - start_time), best_sigma[region]))\n",
    "            \n",
    "    for i in range(0, len(fcma)):\n",
    "        row = (pd.Series({'region': region, 'proposed_schools_n': n,\n",
    "                    'iteration':i, 'algorithm':'CMA', 'ee':round(abs(fcma[i][0]),0),\n",
    "                                'eei':round(abs(ee_old_constant - abs(fcma[i][0])),0), \n",
    "                                'proposed_locations': fcma[i][1], 'time':fcma[i][2], 'sigma':fcma[i][3]}))\n",
    "        results = results.append(row, ignore_index=True)\n",
    "    \n",
    "    print(region, 'CMA complete. Total time:', round(time.time() - start_time_cma,2))\n",
    "    \n",
    "    results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = set(df['ADM1_EN'])\n",
    "for i in all_regions:\n",
    "    run(df,i, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_regions = set(df['ADM1_EN'])\n",
    "for i in all_regions:\n",
    "    run(df,i, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reg2 = all_regions['Harari'].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reg2 = {'Addis Ababa',\n",
    " 'Amhara',\n",
    " 'Benishangul Gumz',\n",
    " 'Dire Dawa',\n",
    " 'Oromia',\n",
    " 'SNNP',\n",
    " 'Somali',\n",
    " 'Tigray'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_reg2:\n",
    "    run(df,i, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(df,'Addis Ababa', 10) # only 8 schools in Harari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df,'SNNP', 10)# only 8 schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = set(df['ADM1_EN'])\n",
    "for i in all_regions:\n",
    "    run(df,i, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(df, 'Harari', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Addis Ababa', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Amhara', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Benishangul Gumz', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(df, 'Dire Dawa', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Oromia', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'SNNP', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Somali', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Tigray', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(df, 'Harari', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Addis Ababa', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Amhara', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Benishangul Gumz', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Dire Dawa', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Oromia', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'SNNP', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Somali', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Tigray', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Amhara', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Oromia', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'SNNP', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Tigray', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Benishangul Gumz', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Harari', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(df, 'Addis Ababa', 10) # Not necessary. \n",
    "run(df, 'Dire Dawa', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    run() # all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df_middle[df_middle['nearest_lwr_sec'] > 5] # Find all PS > 5km distance\n",
    "dd = dd.sort_values(['grade7_8'], ascending=False).head(n) # filter by number of SS to construct i.e. 5.\n",
    "benchmark = sum(dd['grade7_8']) # sum the enrollment for a basic indication of expected enrollment\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above displays the overall enrollment from only the number of proposed schools. However, there is also a need to test these locations using the objective function. Why? In many cases, schools with high enrollment are located in more urban locations which may be close by to many other primary schools that could benefit from a new SS being built nearby the proposed schools. The objective function will provide a wider estimate than only the schools with the top enrollment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_loc = dd['point'].reset_index(drop=True).to_numpy()\n",
    "benchmark_loc = np.array([np.array(i) for i in benchmark_loc], dtype=float)\n",
    "benchmark_f = f(benchmark_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the figures to beat.\n",
    "print('Overall expected enrollment: ', benchmark_f, '\\n' \\\n",
    "      'Expected Enrollment Increase: ', round(abs(np.sum(df_sec_enroll) - abs(benchmark_f)),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Algorothm is used to see if an improved solution can be identified.\n",
    "def random_search(f, n):\n",
    "    x = [generate_sp_proposed(sp, n) for _ in range(n)]\n",
    "    fx = [[f(xi), xi] for xi in x]\n",
    "    best_f, best_solution = min(fx, key=lambda x:x[0])\n",
    "    return best_f, best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_starting_points = 30\n",
    "maxits = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the algorithm 10,000 times by 30 different starting points.\n",
    "fx = []\n",
    "for _ in range(n_starting_points):\n",
    "    start_time = time.time()\n",
    "    fx.append([random_search(f, maxits), time.time() - start_time])\n",
    "    print(_,time.time() - start_time, 'starting point completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search does not find an improved solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = (0.1, 0.4, 0.8, 0.9, 1.2, 1.4, 1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fcma = []\n",
    "for i in range(n_starting_points):\n",
    "    start_time = time.time()\n",
    "    for j in sigmas:\n",
    "        es = cma.CMAEvolutionStrategy(generate_sp_proposed(sp, proposed_schools).flatten(), sigma0=j,\n",
    "                                  inopts={'bounds': boundsxy,'seed':1234})\n",
    "        es.optimize(f, iterations=(maxits/ es.popsize))\n",
    "        fcma.append((es.result[1], es.result[0].reshape(proposed_schools, 2), (time.time() - start_time), j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Potential Good Sigma: ', np.sqrt(np.std(df_prim_loc[:,0])**2 + np.std(df_prim_loc[:,1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = df_prim['point'][df_prim['nearest_lwr_sec'] < 5].reset_index(drop=True).to_numpy()\n",
    "testing = np.array([np.array(i) for i in testing], dtype=float)\n",
    "print('Potential Good Sigma: ', np.sqrt(np.std(testing[:,0])**2 + np.std(testing[:,1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(fcma)):\n",
    "    row = (pd.Series({'random_starting_point':i, 'algorithm':'CMA', 'ee':round(abs(fcma[i][0]),0),\n",
    "                                'eei':round(abs(np.sum(df_sec_enroll) - abs(fcma[i][0])),0), \n",
    "                                'proposed_locations': fcma[i][1], 'time':fcma[i][2], 'sigma':fcma[i][3]}))\n",
    "    results = results.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(['eei'], ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results_revamp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results4 = pd.read_csv('results3.csv', converters={'proposed_locations': literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Show results of top 4.\n",
    "top_4 = results[:4]\n",
    "top_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is latitude then longitude.\n",
    "# box = np.array([[10.713719, 36.689328], [10.713719, 36.96973],[10.964773, 36.96973], [10.964773, 36.689328], [10.713719, 36.689328]])\n",
    "plt.figure(figsize=(15, 10))\n",
    "# plt.plot(box[:,1], box[:,0], '.r-')\n",
    "plt.scatter(df_prim_loc[:, 1], df_prim_loc[:, 0], s=df_prim_enroll/100, label=\"Prim\") # s gives size\n",
    "plt.scatter(df_sec_loc[:, 1], df_sec_loc[:, 0], s=df_sec_enroll/100, label=\"Secondary\") # s gives size\n",
    "plt.scatter(top_4['proposed_locations'][3][:, 1], top_4['proposed_locations'][3][:, 0], s = 35, marker=\"o\", label=\"New Secondary\") # stars for supermarkets\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is latitude then longitude.\n",
    "# box = np.array([[10.713719, 36.689328], [10.713719, 36.96973],[10.964773, 36.96973], [10.964773, 36.689328], [10.713719, 36.689328]])\n",
    "plt.figure(figsize=(15, 10))\n",
    "# plt.plot(box[:,1], box[:,0], '.r-')\n",
    "plt.scatter(df_prim_loc[:, 1], df_prim_loc[:, 0], s=df_prim_enroll/100, label=\"Prim\") # s gives size\n",
    "plt.scatter(df_sec_loc[:, 1], df_sec_loc[:, 0], s=df_sec_enroll/100, label=\"Secondary\") # s gives size\n",
    "plt.scatter(benchmark_loc[:, 1], benchmark_loc[:, 0], s = 35, marker=\"o\", label=\"New Secondary\") # stars for supermarkets\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, figsize=(15,15))\n",
    "fig.suptitle('Top 4 Results. maxits=10,000, random_sp=30')\n",
    "\n",
    "for i in range(4):\n",
    "    ax = 'ax'+str(i)\n",
    "    eval(ax).scatter(df_prim_loc[:, 1], df_prim_loc[:, 0], s=df_prim_enroll/100, label=\"Prim\") # s gives size\n",
    "    if(len(df_sec) != 0): eval(ax).scatter(df_sec_loc[:, 1], df_sec_loc[:, 0], s=df_sec_enroll/100, label=\"Secondary\") # s gives size\n",
    "    eval(ax).scatter([row[1] for row in top_4['proposed_locations'][i]], \n",
    "                     [row[0] for row in top_4['proposed_locations'][i]], s = 35, \\\n",
    "                      marker=\"o\", label=\"New Secondary\") # stars for supermarkets\n",
    "#     eval(ax).scatter(top_4['proposed_locations'][i][:,1], top_4['proposed_locations'][i][:,0], s = 35, \\\n",
    "#                      marker=\"o\", label=\"New Secondary\") # stars for supermarkets\n",
    "    eval(ax).set_title((str(top_4.loc[i]['algorithm']) + ', eei =  ' + str(top_4.loc[i]['eei'])\\\n",
    "                       + ', sigma: ' + str(top_4.loc[i]['sigma'])), fontstyle='italic')\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.legend()\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMA doesn't beat the benchmark. Need to potentially re-run with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner = top_4.head(1)\n",
    "winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shape expected enrollment. \n",
    "def shape2(distance, enrollment):\n",
    "    # If less than 2km, all children expected to attend secondary i.e. distance not a factor\n",
    "    min_walk = 5 \n",
    "    max_walk = 8.94666 # distance greater than 5km (1hr 15 mins) assumes school too far, and zero enrollment expected.\n",
    "    # if between 2-5km, return a linear dropoff.\n",
    "    return np.where(distance<min_walk, enrollment,\n",
    "             np.where(distance>max_walk, 0,\n",
    "                     enrollment*(1-(distance-min_walk)/(max_walk-min_walk)))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_sec_enroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(shape(current_ps_distance, df_prim_enroll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(shape2(current_ps_distance, df_prim_enroll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = cma.CMAEvolutionStrategy(generate_sp_proposed(sp, proposed_schools).flatten(), sigma0=0.9,\n",
    "                          inopts={'bounds': boundsxy,'seed':1234})\n",
    "es.optimize(f, iterations=(maxits/ es.popsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = cma.CMAEvolutionStrategy(benchmark_loc.flatten(), sigma0=0.9,\n",
    "                          inopts={'bounds': boundsxy,'seed':1234})\n",
    "es.optimize(f, iterations=(maxits/ es.popsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prim['urban_rural'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_prim[df_prim['urban_rural'] == 2]['nearest_lwr_sec']) # distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_prim[df_prim['urban_rural'] == 1]['nearest_lwr_sec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(winner['proposed_locations'][0])\n",
    "# np.mean(df_sec_enroll)\n",
    "# update_expected_enrollment(winner['proposed_locations'][0])\n",
    "# np.sum(eei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prim.groupby(['nearest_sch_code'])['expected_enroll'].agg('sum')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = haversine_vector(df_prim_loc,df_sec_loc, Unit.KILOMETERS, comb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dd < 5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
